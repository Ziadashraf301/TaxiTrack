from pathlib import Path
from config import PATH_CONFIG, TABLE_CONFIG
from pipeline import TimeSeriesForecastPipeline
from logger import setup_logger

log_dir = Path(PATH_CONFIG["logs_dir"])
logger = setup_logger(__name__, f"{log_dir}/pipeline.log")


def test_data_loading():
    """Test data loading from ClickHouse"""
    logger.info("ğŸ§ª Testing Data Loading...")
    
    try:
        pipeline = TimeSeriesForecastPipeline()
        pipeline.data_loader.min_hours = 100

        # Test with a small date range first
        df = pipeline.data_loader.get_processed_data(
            table_name=TABLE_CONFIG["table"],
            start_date="2024-01-01",
            end_date="2024-09-01",
        )
        
        logger.info(f"âœ… Data loaded successfully: {len(df)} rows, "
                    f"{len(df.columns)} columns, "
                    f"date range {df['pickup_datetime'].min()} â†’ {df['pickup_datetime'].max()}")
        return df
        
    except Exception as e:
        logger.error(f"âŒ Data loading failed: {e}", exc_info=True)
        return None


def test_feature_engineering(df):
    """Test feature engineering"""
    logger.info("ğŸ§ª Testing Feature Engineering...")
    
    try:
        pipeline = TimeSeriesForecastPipeline()
        df_features = pipeline.feature_engineer.transform(df)
        
        logger.info(f"âœ… Feature engineering successful: "
                    f"{len(df.columns)} â†’ {len(df_features[0].columns)} features")
        return df_features
        
    except Exception as e:
        logger.error(f"âŒ Feature engineering failed: {e}", exc_info=True)
        return None


def test_small_pipeline():
    """Test the complete pipeline with a small dataset"""
    logger.info("ğŸ§ª Testing Complete Pipeline (Small Scale)...")
    
    try:
        pipeline = TimeSeriesForecastPipeline()
        pipeline.data_loader.min_hours = 100

        pipeline.run_pipeline(
            table_name=TABLE_CONFIG["table"],
            start_date="2024-01-01",
            end_date="2024-10-01",
        )
        
        logger.info("âœ… Pipeline completed successfully!")
        
        if hasattr(pipeline, 'results'):
            logger.info("=== MODEL RESULTS SUMMARY ===")
            for model_name, results in pipeline.results.items():
                logger.info(f"{model_name.upper():<15}: "
                            f"Train RMSE: {results['train_rmse']:.2f}, "
                            f"Test RMSE: {results['test_rmse']:.2f}")
        return pipeline
        
    except Exception as e:
        logger.error(f"âŒ Pipeline failed: {e}", exc_info=True)
        return None


def main():
    """Run all tests"""
    logger.info("ğŸš€ Starting Pipeline Tests...")
    logger.info("=" * 50)
    
    df = test_data_loading()
    if df is None:
        logger.error("âŒ Stopping tests - data loading failed")
        return
    
    df_features = test_feature_engineering(df)
    if df_features is None:
        logger.error("âŒ Stopping tests - feature engineering failed")
        return
    
    pipeline = test_small_pipeline()
    
    logger.info("=" * 50)
    if pipeline is not None:
        logger.info("ğŸ‰ ALL TESTS PASSED!")
    else:
        logger.warning("âš ï¸ Some tests failed. Check logs above.")


if __name__ == "__main__":
    main()
